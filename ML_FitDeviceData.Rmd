---
title: "Machine Learning on data from Fitness Trackers Devices"
author: "Shan Dey"
date: "Sunday, August 23, 2015"
output: html_document
---
## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement Â– a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this data set, the participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants toto predict the manner in which praticipants did the exercise.

The dependent variable or response is the “classe” variable in the training set.

## Data
Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. 

Please download the data if you want to try it out.
```{r echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, warning=FALSE}
library(caret)
library(rattle)
library(randomForest)
library(rpart)
library(rpart.plot)

setwd("C:\\Users\\sdey\\Box Sync\\Shan Laptop\\Desk\\Sekha\\R\\MachineLearning")
```

## Get data into Training and Testing sets. Keep the testing set aside for later use.
```{r}
# download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "./pml-training.csv")
# download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "./pml-testing.csv")

set.seed(12345)

trainData = read.csv("pml-training.csv", na.strings=c("", "NA", "NULL"))

testData = read.csv("pml-testing.csv", na.strings=c("", "NA", "NULL"))
dim(trainData)
dim(testData)
```

## Clean Up Data, remove columns / variables with #NAs or near Zero Variance
```{r}
trainData <- trainData[,colSums(is.na(trainData)) == 0]
nearZeroVar <- nearZeroVar(trainData, saveMetrics = TRUE)
trainData <- trainData[, !nearZeroVar$nzv]
trainData <- trainData[,-(1:6)]

dim(trainData)
```

## Split the Train data into Training and Test for Cross Validation before applying on actual test data
```{r}
inTrain <- createDataPartition(trainData$classe, p =0.75, list=FALSE)
trainDataForTrain <- trainData[inTrain,]
trainDataForTest <- trainData[-inTrain,]

dim(trainDataForTrain)
dim(trainDataForTest)

```
## Out of sample error

We will use the model accuracy in the cross validation data.Accuracy is determined by the total number of correct predictions against the test dataset genrated from the original datset. Out of sample error is 1 minus accuracy i.e the expected number of misclassified data.

## Model 1 - Classification Tree
```{r}

modelCF <- train(classe ~., method="rpart", data=trainDataForTrain)
fancyRpartPlot(modelCF$finalModel)

```

## Applying the Model

Apply the model on the same training dataset and check for accuracy.
```{r}
predictionCF <- predict(modelCF, trainDataForTrain)
confusionMatrix(predictionCF, trainDataForTrain$classe)

```
The model accuracy is not so good. So, let's try other model.

## # Model 2 - Random Forest
```{r}

modelRF <- randomForest(classe ~. , data=trainDataForTrain, method=class)
```
## Applying the Model

Apply the model on the same training dataset and check for accuracy.
```{r}

predictionRF <- predict(modelRF, trainDataForTrain)
confusionMatrix(predictionRF, trainDataForTrain$classe)
```
Accuracy looks good, let's test with test /validation set...

## Now try with validation set
Apply the model on the test / validation set from the same training dataset and check for accuracy.
```{r}

predictionRF2 <- predict(modelRF, trainDataForTest)
confusionMatrix(predictionRF2, trainDataForTest$classe)
```

The random Forest model has produced very high accuracy (over 99%) and hence very little out of sample errors. Let's use the random forest model and apply this to our original test dataset. 

## Applying the Model to the test data and product output files

Now apply the model on the test data
```{r}

predictionsRFOnTest <- predict(modelRF, testData, type = "class")
```
## Create the output files per test data
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictionsRFOnTest)

```

